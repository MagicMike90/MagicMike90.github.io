{"componentChunkName":"component---src-templates-blog-post-js","path":"/machine-learning-ensemble-learning","result":{"data":{"site":{"siteMetadata":{"title":"Michael Blog","author":"Michael Luo"}},"markdownRemark":{"id":"e08d6754-5c92-562f-bc30-1e53832a1dbf","excerpt":"Ensemble learning Ensemble methods: combine different classifiers into a meta-classifier that has a better generalization performance than each individual…","html":"<h1>Ensemble learning</h1>\n<p>Ensemble methods: combine different classifiers into a meta-classifier that has a better generalization performance than each individual classifier alone.</p>\n<p>Example: assuming that we collected predictions from 10 experts, ensemble methods would allow us to strategically combine these predictions by the 10 experts to come up with a prediction that is more accurate and robust than the predictions by each individual expert.</p>\n<ul>\n<li>Majority voting: select the class label that has been predicted by the majority of classifiers, that is, received more than 50 percent of the votes.Strictly speaking, it refers to binary class settings only.</li>\n<li>Plurality voting: select the class label that received the most votes.</li>\n</ul>\n<h2>Why using logistic regression and k-nearest neighbors classifier as part of pipleline?</h2>\n<p>Both algorithms(using the Euclidean distance metric) are not scale-invariant in contrast with decision trees.</p>\n<h2>Bagging ( boostrap aggregating)</h2>\n<p>The convertional baggin algorithm involves generating ‘n’ different boostrap training samples with replacement. And training the algorithm on each boostrapped algorithm separately and then aggregating the predictions at the end.</p>\n<p>Bagging is used for reducing overfitting in order to create strong learners for generating accurate predictions. Unlike boosting, baggin allows replacement in the boostrapped sample.</p>\n<ul>\n<li>\n<p>Advantages</p>\n<ul>\n<li>Improves stability &#x26; accuracy of machine leraning algorithms</li>\n<li>Reduces variance</li>\n<li>Overcomes overfitting</li>\n<li>Improved misclassification ratre of the bagged classifier</li>\n<li>In noisy data enviroments bagging outperforms boosting</li>\n</ul>\n</li>\n<li>\n<p>Disavantages</p>\n<ul>\n<li>Bagging works only if the base classifers are not bad to begin with. Bagging bad classifiers can further degrade performance</li>\n</ul>\n</li>\n</ul>\n<h2>Adaptive Boosting (weak learner)</h2>\n<p>Ada Boost is the first original boosting technique which creates a highly accurate prediction rule by combining many weak and inaccurate rules. Each classifer is serially trained with the goal of correcly classifying examples in every round that were incorrectly classified in the previous round.</p>\n<p>For a learned classifier to make strong predictions, it should follow the follwing 3 conditions:</p>\n<ul>\n<li>The rules should be simple</li>\n<li>Classifier should have been trained on sufficient number of training examples.</li>\n<li>The classifer should have low training error for the traning instances</li>\n</ul>\n<p>Advantages</p>\n<ul>\n<li>Very simple to implement</li>\n<li>Good generalization - suited for any kind of classification problem </li>\n</ul>\n<p>Disadvantages</p>\n<ul>\n<li>Sensitive to noisy data and outliers</li>\n</ul>","frontmatter":{"title":"Ensemble learning Notes!","date":"January 31, 2019"}}},"pageContext":{"slug":"machine-learning-ensemble-learning","previous":{"fields":{"slug":"machine-learning-feature-engineering"},"frontmatter":{"title":"Feature Engineering Notes!"}},"next":{"fields":{"slug":"machine-learning-intro"},"frontmatter":{"title":"Introduction of Machine Learning!"}}}}}