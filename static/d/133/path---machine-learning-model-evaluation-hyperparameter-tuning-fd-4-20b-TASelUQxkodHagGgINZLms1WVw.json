{"data":{"site":{"siteMetadata":{"title":"Michael Blog","author":"Michael Luo"}},"markdownRemark":{"id":"af82c780-1028-5592-b58a-6e63d240c732","excerpt":"Model evaluation and Hyperparameter tuning Holdout cross-validation A classic and popular approach for estimating the generalization performance of machine…","html":"<h1>Model evaluation and Hyperparameter tuning</h1>\n<h2>Holdout cross-validation</h2>\n<p>A classic and popular approach for estimating the generalization performance of machine learning models is holdout cross-validation.</p>\n<p><strong>Model selection</strong>: tuning and comparing different paramenter settings to further improve the performance for making predictions on unsee data, it refers to a givent classifcation problem for which we want to select the optimal values of tunning parameters. (<strong>Hyperparameters</strong>)</p>\n<p>A better way of using holdout method for modle selection is to separate the data into three parts: a traning set, a validation set, and a test set.</p>\n<p>The advantage of having a test set that the model hasn’t seen before during the training and model selection steps is that we can objtain a less biased estimate of its ability to generalize to new data.</p>\n<p>A disadvantage of the holdout method is that the preformance estimate is sensitive to how we partition the training set into the training and validation subsets</p>\n<h2>K-fold cross-validation</h2>\n<p>Randomly split the training dataset into k folds without replacement, wheere k - 1 folds are used for the model training and on fold is use for testing. This procedure is repeated k times so that we obtain k models and performance estimates.</p>\n<p>Random sampling without replacement: 2, 1, 3, 4, 0\nRandom sampling with replacement: 1, 3, 3, 4, 1</p>\n<p>Benefits of cross validation:</p>\n<ul>\n<li>each example will be in the test set exactly once</li>\n<li>each example is in one of the folds, and each fold is the test set once</li>\n<li>having multiple splits of data provides some information about how sensitive our model is to the selection of the traning dataset.</li>\n<li>it is more effectively than train<em>test</em>split</li>\n</ul>\n<p>Disadvantage:\nit increase computational cost, since it trains k models instead of a single model, cross validation will be roughly k tims slower than doing a single split of data.</p>\n<h2>Debugging algorithms with learning and validation curves</h2>\n<ul>\n<li>Diagnosing bias and variance problems with learning curves</li>\n<li>Addressing overfitting and underfitting with validation curves</li>\n<li>Fine-tuning machine learning models via grid search</li>\n<li>Algorithm selection with nested cross-validation</li>\n<li>\n<p>Looking at different performance evaluation metrics</p>\n<ul>\n<li>precision</li>\n<li>recall</li>\n<li>F1-score</li>\n<li>Receiver operator charateristic (ROC): this graphs are useful tools for selecting models for classification based on their performance with respect to the false positive and ture positive rates, which are computed by shifting the decision threshold of the classifer.</li>\n<li>Area under the cureve (AUC) can be computed based on ROC curve to charaterize the performance of a classification model</li>\n</ul>\n</li>\n</ul>","frontmatter":{"title":"Model Evaluation Hyperparameter Tuning notest!","date":"January 31, 2019"}}},"pageContext":{"slug":"machine-learning-model-evaluation-hyperparameter-tuning","previous":{"fields":{"slug":"machine-learning-regression-analysis"},"frontmatter":{"title":"Regression Analysis notest!"}},"next":{"fields":{"slug":"machine-learning-feature-engineering"},"frontmatter":{"title":"Feature Engineering Notes!"}}}}