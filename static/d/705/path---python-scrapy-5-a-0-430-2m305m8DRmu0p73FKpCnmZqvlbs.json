{"data":{"site":{"siteMetadata":{"title":"Michael Blog","author":"Michael Luo"}},"markdownRemark":{"id":"f0ecfb1c-d6d7-5401-a902-b9b017834b82","excerpt":"Scrapy notes Database choice If you plan to build some kind of search engine later on, Elasticsearch was a good choice If on the other hand you just want to…","html":"<h1>Scrapy notes</h1>\n<h2>Database choice</h2>\n<ul>\n<li>If you plan to build some kind of search engine later on, Elasticsearch was a good choice</li>\n<li>If on the other hand you just want to store the data and do processing tasks on it later, Elasticsearch was a poor choice and you would be better off with Cassandra or another NoSQL database.</li>\n</ul>\n<h2>Important notes</h2>\n<ol>\n<li>Python twisted</li>\n<li>Using MySQL instead of MongoDB due to some of the drawbacks that when using MongoDB as Database for data scraping.</li>\n<li>Using large pool of IP (proxy) to route requests</li>\n<li>Increasing request delay</li>\n<li>Reduce number of requests per second</li>\n</ol>\n<h3>A table that can help you decide what the best mechanism for a given problem is:</h3>\n<table>\n<thead>\n<tr>\n<th><strong>Problem</strong></th>\n<th align=\"center\"><strong>Solution</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><em>Something that is specific to the website that I’m crawling.</em></td>\n<td align=\"center\"><code class=\"language-text\">Modify your Spider.</code></td>\n</tr>\n<tr>\n<td><em>Modifying or storing Items—domain-specific, may be reused across projects.</em></td>\n<td align=\"center\"><code class=\"language-text\">Write an Item Pipeline.</code></td>\n</tr>\n<tr>\n<td><em>Modifying or dropping Requests/Responses—domain-specific,may be reused across projects</em></td>\n<td align=\"center\"><code class=\"language-text\">Write a spider middleware.</code></td>\n</tr>\n<tr>\n<td><em>Executing Requests/Responses—generic, for example,to support some custom login scheme or a special way to handle cookies.</em></td>\n<td align=\"center\"><code class=\"language-text\">Write a downloader middleware.</code></td>\n</tr>\n<tr>\n<td><em>All other problems.</em></td>\n<td align=\"center\"><code class=\"language-text\">Write an extension.</code></td>\n</tr>\n</tbody>\n</table>\n<h1>Performance</h1>\n<ol>\n<li>\n<p>Find the bottleneck</p>\n</li>\n<li>\n<p>lantencies: t(download) = t(response) + t(overhead)</p>\n<ul>\n<li>t(job) = (N(request) * (t(response) + t(overhead))) / CONCURRENT_REQUESTS + t(start/stop)</li>\n</ul>\n</li>\n<li>\n<p>Parameters control</p>\n<ul>\n<li>t(overhead): more powerful server</li>\n<li>t(start/stop) : same as above</li>\n</ul>\n</li>\n<li>\n<p>Caculate the throughput</p>\n<ul>\n<li>T = N/(t(job) - t(start/stop))</li>\n<li>By running a long job of N Requests, we can measure the job t aggregated time and then it’s straightforward to calculate T.</li>\n</ul>\n</li>\n</ol>\n<h2>Case study:</h2>\n<h3>1. saturated CPU</h3>\n<p><strong>Solution</strong>: I will assume that your code is, in general, efficient. You can get\naggregated concurrency larger than <em>CONCURRENT</em>REQUESTS_ by running many Scrapy\ncrawlers on the same server. This will help you utilize more of the available cores\nespecially if other services or other threads from your pipelines don’t use them.\nIf you need even more concurrency, you can use multiple servers (see Chapter 11,\nDistributed Crawling with Scrapyd and Real-Time Analytics), in which case you will\nlikely have more memory, network bandwidth, and hard disk throughput available\nas well. Always double-check that CPU usage is your primary constraint.</p>\n<h3>2. blocking code</h3>\n<p><strong>Solution</strong>: I will assume that you inherited the code base, and you have no intuition\non where the blocking code is. If the system can be functional without any pipelines,\nthen disable your pipelines and check whether the odd behavior persists. If yes, then\nyour blocking code is in your spider. If not, then enable pipelines one-by-one and\nsee when the problem starts. If the system can’t be functional without everything\nrunning, then add some log messages on each pipeline stage (or interleave dummy\npipelines that print timestamps) in between your functional ones. By checking the\nlogs, you will easily detect where your system spends most of its time. If you want\na more long-term/reusable solution, you can trace your Requests using dummy\npipelines that add timestamps at each stage to the meta fields of Request. At the end,\nhook to the item<em>scraped signal and log the timestamps. As soon as you find your\nblocking code, convert it to Twisted/asynchronous or use Twisted’s thread pools. To\nsee the effects of this conversion, rerun the previous example while replacing SPEED</em>\n<em>PIPELINE</em>BLOCKING<em>DELAY</em> with <em>SPEED</em>PIPELINE<em>ASYNC</em>DELAY_. The change in\nperformance is stunning.</p>\n<h3>3. “garbage” on the downloader</h3>\n<p><strong>Solution</strong>: We can solve this problem using treq instead of <em>crawler.engine.\ndownload().</em> You will note that this will skyrocket the scraper’s performance, which\nmight be bad news for your API infrastructure. I would start with a low number\nof <em>CONCURRENT</em>REQUESTS_ and increase gradually to make sure I don’t overload the\nAPI servers.</p>\n<h3>4. overflow due to many or large responses</h3>\n<p><strong>Solution</strong>: There isn’t much you can do for this problem with the existing infrastructure.\nIt would be nice to be able to clear the body of Response as soon as you don’t need it\nanymore—likely after your spider, but doing so won’t reset Scraper’s counters at the\ntime of writing. All you can really do is try to reduce your pipeline’s processing time\neffectively reducing the number of Responses in progress in the Scraper. You can\nachieve this with traditional optimization: checking whether APIs or databases you\npotentially interact with can support your scraper’s throughput, profiling the scraper,\nmoving functionality from your pipelines to batch/postprocessing systems, and\npotentially using more powerful servers or distributed crawling.</p>\n<h3>5. overflow due to limited/excessive item concurrency</h3>\n<p><strong>Solution</strong>: It’s very easy to detect both problematic symptoms of this case. If you\nget very high CPU usage, it’s good to reduce the number of CONCURRENT<em>ITEMS. If\nyou hit the 5 MB Response limit, then your pipeline can’t follow your downloader’s\nthroughput and increasing CONCURRENT</em>ITEMS might be able to quickly fix this. If it\ndoesn’t make any difference, then follow the advice in the previous section and ask\nyourself twice if the rest of the system is able to support your Scraper’s throughput.</p>\n<h3>6. the downloader doesn’t have enough to do</h3>\n<p><strong>Solution</strong>: If each index page has more than one next page link, we can utilize them\nto accelerate our URL generation. If we can find pages that show more results (for\nexample, 50) per index page even better.</p>\n<h2>Troubleshooting flow</h2>\n<p>To summarize, Scrapy is designed to have the downloader as a bottleneck. Start with\na low value of CONCURRENT_REQUESTS and increase until just before you hit one of\nthe following limits:</p>\n<ul>\n<li>CPU usage > 80-90%</li>\n<li>Source website latency increasing excessively</li>\n<li>Memory limit of 5 Mb of Responses in your scraper</li>\n</ul>\n<p>At the same time also perform the following:</p>\n<ul>\n<li>Keep at least a few Requests at all times in the scheduler’s queues (mqs/dqs)\nto prevent the downloader’s URL starvation</li>\n<li>Never use any blocking code or CPU-intensive code</li>\n</ul>\n<p><img src=\"https://github.com/MagicMike90/Notes/blob/master/Python/Troubleshooting%20Scrapy&#x27;s%20performance%20problems.png?raw=true\" alt=\"Troubleshooting Scrapy&#x27;s performance problems\"></p>\n<h2>Scrapy Distibuted Crawling</h2>\n<h2>Crawl steps</h2>\n<ul>\n<li>Inspect website structres</li>\n<li>Use different spiders in order to satify website current structures</li>\n<li>Using another application to schedule scrapy crawl</li>\n</ul>\n<h2>Error</h2>\n<p>Offsite: Check <code class=\"language-text\">allowed_domains</code> if it matches the domain will be scraped</p>\n<h2>Setting</h2>\n<p>Using large pool of IP (proxy) to route requests\nIncreasing request delay\nReduce number of requests per second</p>\n<h2>Environment</h2>\n<p>AWS / ScrapyHub\nRadis, MySQL / MogoDB\nElastic search\nSpark</p>\n<h2>Advance</h2>\n<p>Distributed Crawling with multiple servers</p>","frontmatter":{"title":"Notes for Scrapy!","date":"February 26, 2019"}}},"pageContext":{"slug":"python-scrapy","previous":{"fields":{"slug":"frontend"},"frontmatter":{"title":"Frontend Notes!"}},"next":{"fields":{"slug":"react"},"frontmatter":{"title":"React Notes!"}}}}