{"data":{"site":{"siteMetadata":{"title":"Michael Blog","author":"Michael Luo"}},"markdownRemark":{"id":"eec4d0f1-f01f-57a2-965e-a61ea09e306d","excerpt":"Training Algorithms Linear Algebra notes Numpy The advantage of using Numpy over classic Python for-loop structures is that its arithmetic operations are…","html":"<h1>Training Algorithms</h1>\n<h2><a href=\"http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf\">Linear Algebra notes</a></h2>\n<h2>Numpy</h2>\n<ul>\n<li>The advantage of using Numpy over classic Python for-loop structures is that its arithmetic operations are vectorized, imporve the performance of operations by making better use of <strong>Single Instruction, Multiple Data (SIMD)</strong> of moden CPU architecures.</li>\n</ul>\n<p>methods</p>\n<ul>\n<li><strong>dot()</strong>:dot product of two pioints</li>\n<li><strong>zip()</strong>:The zip() function take iterables (can be zero or more), makes iterator that aggregates elements based on the iterables passed, and returns an iterator of tuples.</li>\n<li><strong>where</strong>(condition[, x, y]): Return elements, either from x or y, depending on condition.i.e. np.where(y == ‘Iris-setosa’, -1, 1), where value equals ‘Iris-setosa’ will change to be -1.</li>\n</ul>\n<h2>Pandas</h2>\n<p>methods</p>\n<ul>\n<li><strong>read_csv()</strong>: read csv file , return <em>DataFrame</em></li>\n<li><strong>tail()</strong>: get last n line of DataFrame</li>\n<li><strong>iloc(array, index)</strong>: get numer of rows of column index</li>\n</ul>\n<h2>Disadvantage of perceptron</h2>\n<p>its biggest disadvantage is that it never converges if the classes are not perfectly linearly separable.</p>\n<h2>The curse of dimensionality</h2>\n<p>The curse of dimensionality describes the phenomenon where the feature\nspace becomes increasingly sparse for an increasing number\nof dimensions of a fixed-size training dataset</p>\n<p><em>Regularization</em> is not applicable such as decision trees and KNN, we can use feature selection and dimensionality reduction techniques to help us avoid the curse of dimensionality</p>","frontmatter":{"title":"Training Algorithms of machine learning!","date":"January 31, 2019"}}},"pageContext":{"slug":"machine-learning-training-algorithms","previous":{"fields":{"slug":"machine-learning-sentiment-analysis"},"frontmatter":{"title":"Sentiment Analysis notest!"}},"next":{"fields":{"slug":"machine-learning-summary"},"frontmatter":{"title":"Summary of machine learning!"}}}}